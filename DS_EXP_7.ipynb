{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614a2d74-09a7-4001-b02a-7e8aaadef151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d01cbbd-4436-4960-9ff6-a722d4086002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'whats', 'app']\n",
      "Help on function stem in module nltk.stem.porter:\n",
      "\n",
      "stem(self, word, to_lowercase=True)\n",
      "    :param to_lowercase: if `to_lowercase=True` the word always lowercase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(word_tokenize)\n",
    "\n",
    "tokens = word_tokenize(\"Hello whats app\")\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# help(PorterStemmer)\n",
    "# dir(nltk.stem)\n",
    "\n",
    "# dir(PorterStemmer.stem)\n",
    "help(PorterStemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5409fe7-2859-48ac-bc85-a506d5eb446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: Tokenization is the process of splitting text into words and sentences. POS tagging assigns a part-of-speech tag to each word. Stopwords removal removes common words like 'the', 'is', and 'and'. Stemming reduces words to their root form, while lemmatization reduces words to their base form.\n",
      "Tokens: ['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'words', 'and', 'sentences', '.', 'POS', 'tagging', 'assigns', 'a', 'part-of-speech', 'tag', 'to', 'each', 'word', '.', 'Stopwords', 'removal', 'removes', 'common', 'words', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', 'and', \"'and\", \"'\", '.', 'Stemming', 'reduces', 'words', 'to', 'their', 'root', 'form', ',', 'while', 'lemmatization', 'reduces', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "POS Tags: [('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('splitting', 'VBG'), ('text', 'NN'), ('into', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('sentences', 'NNS'), ('.', '.'), ('POS', 'NNP'), ('tagging', 'VBG'), ('assigns', 'RP'), ('a', 'DT'), ('part-of-speech', 'JJ'), ('tag', 'NN'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN'), ('.', '.'), ('Stopwords', 'NNS'), ('removal', 'JJ'), ('removes', 'NNS'), ('common', 'JJ'), ('words', 'NNS'), ('like', 'IN'), (\"'the\", 'NNP'), (\"'\", 'POS'), (',', ','), (\"'is\", 'NNP'), (\"'\", 'POS'), (',', ','), ('and', 'CC'), (\"'and\", 'NNP'), (\"'\", 'POS'), ('.', '.'), ('Stemming', 'VBG'), ('reduces', 'NNS'), ('words', 'NNS'), ('to', 'TO'), ('their', 'PRP$'), ('root', 'NN'), ('form', 'NN'), (',', ','), ('while', 'IN'), ('lemmatization', 'NN'), ('reduces', 'NNS'), ('words', 'NNS'), ('to', 'TO'), ('their', 'PRP$'), ('base', 'NN'), ('form', 'NN'), ('.', '.')]\n",
      "Filtered Tokens (Stopwords Removed): ['Tokenization', 'process', 'splitting', 'text', 'words', 'sentences', '.', 'POS', 'tagging', 'assigns', 'part-of-speech', 'tag', 'word', '.', 'Stopwords', 'removal', 'removes', 'common', 'words', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', \"'and\", \"'\", '.', 'Stemming', 'reduces', 'words', 'root', 'form', ',', 'lemmatization', 'reduces', 'words', 'base', 'form', '.']\n",
      "Stemmed Tokens: ['token', 'process', 'split', 'text', 'word', 'sentenc', '.', 'po', 'tag', 'assign', 'part-of-speech', 'tag', 'word', '.', 'stopword', 'remov', 'remov', 'common', 'word', 'like', \"'the\", \"'\", ',', \"'i\", \"'\", ',', \"'and\", \"'\", '.', 'stem', 'reduc', 'word', 'root', 'form', ',', 'lemmat', 'reduc', 'word', 'base', 'form', '.']\n",
      "Lemmatized Tokens: ['Tokenization', 'process', 'splitting', 'text', 'word', 'sentence', '.', 'POS', 'tagging', 'assigns', 'part-of-speech', 'tag', 'word', '.', 'Stopwords', 'removal', 'remove', 'common', 'word', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', \"'and\", \"'\", '.', 'Stemming', 'reduces', 'word', 'root', 'form', ',', 'lemmatization', 'reduces', 'word', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample document\n",
    "document = \"Tokenization is the process of splitting text into words and sentences. POS tagging assigns a part-of-speech tag to each word. Stopwords removal removes common words like 'the', 'is', and 'and'. Stemming reduces words to their root form, while lemmatization reduces words to their base form.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(\"Original Document:\", document)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Filtered Tokens (Stopwords Removed):\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efabf4-2b31-4bf2-a69b-9b2360a08f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
